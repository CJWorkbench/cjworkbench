# Generated by Django 2.2.17 on 2020-12-17 20:41

from typing import Iterable, List, Tuple

from django.db import migrations

# from cjworkbench/settings.py, 2020-12-17
MAX_BYTES_FETCHES_PER_STEP = 1024 * 1024 * 1024
MAX_N_FETCHES_PER_STEP = 30
MAX_N_FILES_PER_STEP = 30
MAX_BYTES_FILES_PER_STEP = 2 * 1024 * 1024 * 1024


# from cjwstate/util.py, 2020-12-17
def find_deletable_ids(
    *, ids_and_sizes: Iterable[Tuple[int, int]], n_limit: int, size_limit: int
) -> List[int]:
    """Given a sequence of object IDs and their sizes, choose some to delete.

    Rules (higher rule trumps lower rules):

        * Never return the first object's ID. It's "sacred".
        * Accumulate `size`; return all IDs once it exceeds `size_limit`.
        * Accumulate `n` (number of IDs); return all IDs past `n_limit`.
    """
    # walk over files from newest to oldest, deleting once we're too far.
    sum_size = 0
    ret = []
    for i, (id, size) in enumerate(ids_and_sizes):
        sum_size += size
        if i > 0:
            if (
                i >= n_limit  # too many files -- costs in # of operations
                or sum_size > size_limit  # too many GB -- costs in GB/mo
            ):
                ret.append(id)
    return ret


# from cjwstate/storedobjects/io.py, 2020-12-17
def delete_excess_stored_objects(*, step: "Step") -> None:
    """Delete old fetches that bring us past MAX_BYTES_FETCHES_PER_STEP or
    MAX_N_FETCHES_PER_STEP.

    We can't let every workflow grow forever.
    """
    to_delete = find_deletable_ids(
        ids_and_sizes=step.stored_objects.order_by("-stored_at").values_list(
            "id", "size"
        ),
        n_limit=MAX_N_FETCHES_PER_STEP,
        size_limit=MAX_BYTES_FETCHES_PER_STEP,
    )
    if to_delete:
        from cjwstate import minio

        for stored_object in step.stored_objects.filter(id__in=to_delete):
            print(
                "  Step %d, StoredObject %d (%d bytes)"
                % (step.id, stored_object.id, stored_object.size)
            )
            if stored_object.key:
                minio.remove(minio.StoredObjectsBucket, stored_object.key)
            stored_object.delete()


# from cjwstate/models/uploaded_file.py, 2020-12-17
def delete_excess_user_files(*, step: "Step") -> int:
    """Delete old fetches that bring us past MAX_BYTES_FILES_PER_STEP or
    MAX_N_FILES_PER_STEP.

    We can't let every workflow grow forever.

    Return number of files deleted. If this isn't 0, the caller must send a
    clientside.Update with the new `files` list.
    """
    to_delete = find_deletable_ids(
        ids_and_sizes=step.uploaded_files.order_by("-created_at").values_list(
            "id", "size"
        ),
        n_limit=MAX_N_FILES_PER_STEP,
        size_limit=MAX_BYTES_FILES_PER_STEP,
    )
    if to_delete:
        from cjwstate import minio

        for uploaded_file in step.uploaded_files.filter(id__in=to_delete):
            print(
                "  Step %d, UploadedFile %d (%d bytes)"
                % (step.id, uploaded_file.id, uploaded_file.size)
            )
            minio.remove(minio.UserFilesBucket, uploaded_file.key)
            uploaded_file.delete()


def slowly_enforce_storage_limits(apps, schema_editor):
    """Delete old files that exceed our new storage restrictions.

    Bugs and strange decisions:

    * We don't send websockets notifications.
      Consequence: users might see an error popup and need to refresh the page.
    * We might theoretically delete a _selected_ file.
      Consequence: if a re-render happens, the data will disappear.
    * There are no transactions. This should help: every resume will bring us
      closer to success.
    """
    # non-atomic migrations example:
    # https://docs.djangoproject.com/en/3.1/howto/writing-migrations/#non-atomic-migrations
    from django.db.models import Count, Sum, Q

    print()  # newline, so first line of logs is on its own line

    Step = apps.get_model("server", "Step")
    for step in (
        Step.objects.annotate(n_files=Count("uploaded_files"))
        .annotate(size_files=Sum("uploaded_files__size"))
        .filter(
            Q(n_files__gt=MAX_N_FILES_PER_STEP)
            | Q(size_files__gt=MAX_BYTES_FILES_PER_STEP)
        )
    ):
        print("Deleting some user_files from step %d…" % step.id)
        delete_excess_user_files(step=step)

    for step in (
        Step.objects.annotate(n_objects=Count("stored_objects"))
        .annotate(size_objects=Sum("stored_objects__size"))
        .filter(
            Q(n_objects__gt=MAX_N_FILES_PER_STEP)
            | Q(size_objects__gt=MAX_BYTES_FILES_PER_STEP)
        )
    ):
        print("Deleting some stored_objects from step %d…" % step.id)
        delete_excess_stored_objects(step=step)


class Migration(migrations.Migration):
    atomic = False  # this may take minutes/hours

    dependencies = [
        ("server", "0024_auto_20201103_1653"),
    ]

    operations = [migrations.RunPython(slowly_enforce_storage_limits, elidable=True)]
