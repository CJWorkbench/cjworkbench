# Generated by Django 2.2.17 on 2020-12-17 20:41

from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Iterable, List, Tuple

from django.db import migrations
from django.db.models import Count, Q, QuerySet, Sum

# from cjworkbench/settings.py, 2020-12-17
MAX_BYTES_FETCHES_PER_STEP = 1024 * 1024 * 1024
MAX_N_FETCHES_PER_STEP = 30
MAX_N_FILES_PER_STEP = 30
MAX_BYTES_FILES_PER_STEP = 2 * 1024 * 1024 * 1024


# from cjwstate/util.py, 2020-12-17
def find_deletable_ids(
    *, ids_and_sizes: Iterable[Tuple[int, int]], n_limit: int, size_limit: int
) -> List[int]:
    """Given a sequence of object IDs and their sizes, choose some to delete.

    Rules (higher rule trumps lower rules):

        * Never return the first object's ID. It's "sacred".
        * Accumulate `size`; return all IDs once it exceeds `size_limit`.
        * Accumulate `n` (number of IDs); return all IDs past `n_limit`.
    """
    # walk over files from newest to oldest, deleting once we're too far.
    sum_size = 0
    ret = []
    for i, (id, size) in enumerate(ids_and_sizes):
        sum_size += size
        if i > 0:
            if (
                i >= n_limit  # too many files -- costs in # of operations
                or sum_size > size_limit  # too many GB -- costs in GB/mo
            ):
                ret.append(id)
    return ret


def delete_excess_object(
    bucket: str, query_set: QuerySet, key: str, db_id: int
) -> None:
    """Delete `key` from minio; then delete `db_id` from query_set.

    This signature is designed to be to be used with `map()`.
    """
    # Delete from minio
    if key:  # key="" in some StoredObjects with size=0
        from cjwstate import minio

        minio.remove(bucket, key)
    # Delete from DB, after minio key is removed
    query_set.filter(id=db_id).delete()


def delete_excess_objects(
    *,
    executor: ThreadPoolExecutor,
    bucket: str,
    query_set: QuerySet,
    db_ids: Iterable[int],
) -> None:
    """Delete all keys and objects in query_set with a matching ID.

    `query_set` must point to a table with `key` and `id` columns.
    """
    # Schedule all the jobs (max 100k on production)
    futures = [
        executor.submit(delete_excess_object, bucket, query_set, *key_and_id)
        for key_and_id in query_set.filter(id__in=db_ids).values_list("key", "id")
    ]
    for future in as_completed(futures):
        future.result()  # raise on error


# from cjwstate/storedobjects/io.py, 2020-12-17
def delete_excess_stored_objects(*, executor: ThreadPoolExecutor, step: "Step") -> None:
    """Delete old fetches that bring us past MAX_BYTES_FETCHES_PER_STEP or
    MAX_N_FETCHES_PER_STEP.

    We can't let every workflow grow forever.
    """
    to_delete = find_deletable_ids(
        ids_and_sizes=step.stored_objects.order_by("-stored_at").values_list(
            "id", "size"
        ),
        n_limit=MAX_N_FETCHES_PER_STEP,
        size_limit=MAX_BYTES_FETCHES_PER_STEP,
    )
    if to_delete:
        from cjwstate import minio

        print("Step %d: delete %d StoredObjects" % (step.id, len(to_delete)))
        delete_excess_objects(
            executor=executor,
            bucket=minio.StoredObjectsBucket,
            query_set=step.stored_objects,
            db_ids=to_delete,
        )


# from cjwstate/models/uploaded_file.py, 2020-12-17
def delete_excess_user_files(*, executor: ThreadPoolExecutor, step: "Step") -> None:
    """Delete old fetches that bring us past MAX_BYTES_FILES_PER_STEP or
    MAX_N_FILES_PER_STEP.

    We can't let every workflow grow forever.

    Return number of files deleted. If this isn't 0, the caller must send a
    clientside.Update with the new `files` list.
    """
    to_delete = find_deletable_ids(
        ids_and_sizes=step.uploaded_files.order_by("-created_at").values_list(
            "id", "size"
        ),
        n_limit=MAX_N_FILES_PER_STEP,
        size_limit=MAX_BYTES_FILES_PER_STEP,
    )
    if to_delete:
        from cjwstate import minio

        print("Step %d: delete %d UploadedFiles" % (step.id, len(to_delete)))
        delete_excess_objects(
            executor=executor,
            bucket=minio.UserFilesBucket,
            query_set=step.uploaded_files,
            db_ids=to_delete,
        )


def slowly_enforce_storage_limits(apps, schema_editor):
    """Delete old files that exceed our new storage restrictions.

    Bugs and strange decisions:

    * We don't send websockets notifications.
      Consequence: users might see an error popup and need to refresh the page.
    * We might theoretically delete a _selected_ file.
      Consequence: if a re-render happens, the data will disappear.
    * There are no transactions. This should help: every resume will bring us
      closer to success.
    """
    # non-atomic migrations example:
    # https://docs.djangoproject.com/en/3.1/howto/writing-migrations/#non-atomic-migrations
    print()  # newline, so first line of logs is on its own line
    Step = apps.get_model("server", "Step")

    with ThreadPoolExecutor(max_workers=20) as executor:
        objs = (
            Step.objects.annotate(n_files=Count("uploaded_files"))
            .annotate(size_files=Sum("uploaded_files__size"))
            .filter(
                Q(n_files__gt=MAX_N_FILES_PER_STEP)
                | Q(size_files__gt=MAX_BYTES_FILES_PER_STEP)
            )
            .order_by("id")
        )
        for step in objs:
            delete_excess_user_files(executor=executor, step=step)

        for step in (
            Step.objects.annotate(n_objects=Count("stored_objects"))
            .annotate(size_objects=Sum("stored_objects__size"))
            .filter(
                Q(n_objects__gt=MAX_N_FILES_PER_STEP)
                | Q(size_objects__gt=MAX_BYTES_FILES_PER_STEP)
            )
            .order_by("id")
        ):
            delete_excess_stored_objects(executor=executor, step=step)


class Migration(migrations.Migration):
    atomic = False  # this may take minutes/hours

    dependencies = [
        ("server", "0024_auto_20201103_1653"),
    ]

    operations = [migrations.RunPython(slowly_enforce_storage_limits, elidable=True)]
